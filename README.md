# Readmit-Test
# üè´ DePaul SIS Automation: Student Records & Service Indicators Scraper

Automates data collection from DePaul's **Campus Connect (PeopleSoft)** using **Selenium**.  
This script logs in once (manually), then iterates through a CSV of Student IDs to extract:

- **Academic Term History** ‚Üí *Current GPA*
- **Program/Plan** ‚Üí *Status, Effective Date, Program Action, Action Reason, Academic Program, Academic Plan*
- **Service Indicators** ‚Üí *Code, Description, Reason, Start Date*

> ‚úÖ Outcome: Reduced manual lookups by ~**60%** for large student batches.

---

## üöÄ Features

- üîê **Single sign-on**: you log in manually once; the script handles the rest
- üß≠ **Robust navigation** across multiple PeopleSoft pages (Term History, Program/Plan, Service Indicators)
- ‚è≥ **Smart waits** with `WebDriverWait` (fewer brittle sleeps)
- üßº **Input cleaning**: handles BOM/odd characters in CSV (e.g., `\ufeff`, `ÔøΩ`)
- üìù **CSV output** with stable column order (ready for analysis/import)
- üß© **PeopleSoft‚Äëfriendly**: preserves `#ICSearch` button selector convention (as preferred)

---

## üóÇ Project Structure

```text
sis-automation/
‚îÇ
‚îú‚îÄ‚îÄ scraper.py                # Main script (the one in this README)
‚îú‚îÄ‚îÄ student_data.csv          # Input list of Student IDs (one per line)
‚îú‚îÄ‚îÄ readmit.csv               # Output file with results
‚îî‚îÄ‚îÄ README.md                 # This file
```

---

## üß∞ Requirements

- **Python** 3.9+
- **Google Chrome** (latest) and matching **ChromeDriver** (auto-managed)
- **Packages** (install with `pip install -r requirements.txt`):
  - `selenium`
  - `webdriver-manager`
  - `pandas` (optional; not required by the base script but useful for QA)
  - `python-dotenv` (optional)

**Example `requirements.txt`:**
```txt
selenium>=4.23.1
webdriver-manager>=4.0.2
pandas>=2.2.2
python-dotenv>=1.0.1
```

---

## üîë Setup

1. **Clone** or copy the repo contents locally.
2. **Create** your input CSV (one student ID per line) at:
   ```text
   /Users/<you>/Documents/python/readmit/student_data.csv
   ```
   *The script expects this path; adjust in code if needed.*
3. (Optional) Create a virtual env and install deps:
   ```bash
   python -m venv .venv
   source .venv/bin/activate   # macOS/Linux
   .venv\Scripts\activate      # Windows
   pip install -r requirements.txt
   ```

---

## üîß Configuration

Paths used in the script (adjust if desired):
```python
students = load_students('/Users/rakulsk/Documents/python/readmit/student_data.csv')
output_path = '/Users/rakulsk/Documents/python/readmit/readmit.csv'
```
Chromedriver is auto-installed and kept up to date via `webdriver_manager`.

To run in **headless** mode, add before creating the driver:
```python
options.add_argument("--headless=new")
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")
```

---

## ‚ñ∂Ô∏è Usage

1. **Run the script**:
   ```bash
   python scraper.py
   ```

2. When the login page opens, **log in manually** to Campus Connect:
   - URL (auto): `https://campusconnect.depaul.edu/psp/CSPRD92/?cmd=login`
   - After successful login, **return to the console** and press **Enter** to continue.

3. The script will iterate over your `student_data.csv`, navigating to:
   - **Term History** ‚Üí gets **GPA**
   - **Program/Plan** ‚Üí gets **Status**, **Effective Date**, **Program Action**, **Action Reason**, **Academic Program**, **Academic Plan**
   - **Service Indicators** ‚Üí captures **Code**, **Description**, **Reason**, **Active Date**

4. Results are written to:
   ```text
   /Users/<you>/Documents/python/readmit/readmit.csv
   ```

---

## üìÑ Input / Output

### Input: `student_data.csv`
- Format: **one Student ID per line**
- Example:
  ```csv
  1234567
  9998888
  1002003
  ```

### Output: `readmit.csv`
- Primary rows (from Term History + Program/Plan):
  ```csv
  Student ID,GPA,Status,Effective Date,Program Action,Action Reason,Academic Program,Academic Plan
  1234567,3.61,Active,2024-09-01,PRGC,ADM,LAUG,Undergraduate ‚Äì Data Science
  ```

- Followed by **Service Indicator** rows per student (same CSV), for easy grouping:
  ```csv
  1234567,SI_CODE,SI_DESCRIPTION,REASON_DESCRIPTION,ACTIVE_DATE
  1234567,XW,Financial Hold,Past Due,2024-08-15
  1234567,AD,Advising Required,New Admit,2024-09-01
  ```

> Note: Service indicator rows are written after each student's main row(s), reusing `Student ID` in column 1 for easy joins.

---

## üß© Key Implementation Notes

- **Selectors**: this project **intentionally keeps** the PeopleSoft **`#ICSearch`** button ID, per team preference.
- **Frames**: Campus Connect pages often require switching into `ptifrmtgtframe`:
  ```python
  wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "ptifrmtgtframe")))
  ```
- **Resilience**: helper `safe_get_text()` returns `"N/A"` for missing elements to avoid crashes.
- **Pagination / Tables**: Service Indicator rows are scraped with a simple index loop until `NoSuchElementException` ends the loop.

---

## üõ°Ô∏è Security & Compliance

- The script **does not** store credentials.
- You **log in manually**; the session cookie is used only while the browser remains open.
- Treat all outputs as **FERPA-protected** where applicable. Secure/limit access to `readmit.csv`.

---

## üß™ Troubleshooting

- **Blank values** or **timeouts**:
  - Ensure the iframe switch to `ptifrmtgtframe` succeeded.
  - Increase waits: `WebDriverWait(browser, 45)`.
- **Login loops**:
  - Complete DUO/SSO steps fully before pressing Enter.
- **UI changes**:
  - PeopleSoft upgrades may change element IDs slightly; re-check selectors like `STDNT_SRCH_EMPLID`, `ACAD_PROG_*`, or `SRVC_IND_*`.
- **Keeps clicking wrong things**:
  - Consider scoping with more specific locators and adding `scrollIntoView` via JS.

---

## üìà Impact

- Reduced manual processing time by **~60%** for large student cohorts.
- Standardized output simplifies downstream import/analysis and audit.

---

## ‚úÖ Roadmap / Ideas

- [ ] Write service indicators to a **separate CSV** with explicit headers
- [ ] Add **retry/backoff** for transient load errors
- [ ] Optional **headless** mode toggle via CLI arg (e.g., `--headless`)
- [ ] Add **structured logs** (CSV/JSON) for failures and durations
- [ ] Build a small **Streamlit dashboard** to upload input list and download results

---

## üìú License

MIT ¬© 2025 Rakul Senthilkumar
